{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC=pd.read_csv('Womens Clothing E-Commerce Reviews.csv')\n",
    "WC.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC=WC.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC.dropna(subset=['Review Text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['Recommended IND']=np.where(WC['Recommended IND']==1,0,1)  # 0--recommendation, 1-- not recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22640 entries, 0 to 23485\n",
      "Data columns (total 10 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   Clothing ID              22640 non-null  int64 \n",
      " 1   Age                      22640 non-null  int64 \n",
      " 2   Title                    19675 non-null  object\n",
      " 3   Review Text              22640 non-null  object\n",
      " 4   Rating                   22640 non-null  int64 \n",
      " 5   Recommended IND          22640 non-null  int32 \n",
      " 6   Positive Feedback Count  22640 non-null  int64 \n",
      " 7   Division Name            22627 non-null  object\n",
      " 8   Department Name          22627 non-null  object\n",
      " 9   Class Name               22627 non-null  object\n",
      "dtypes: int32(1), int64(4), object(5)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "WC.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18539\n",
       "1     4101\n",
       "Name: Recommended IND, dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC['Recommended IND'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean  and preprocess reveiw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_split(data):\n",
    "    data=re.sub(\"n't\",'not',data)\n",
    "    data=re.sub(\"i'm\",'i am',data)\n",
    "    data=re.sub(\"I'm\",\"I am\",data)\n",
    "    data=re.sub(\"it's\",\"it is\",data)\n",
    "    data=re.sub(\"It's\",\"It is\",data)\n",
    "    return data\n",
    "WC['Review Text']=WC['Review Text'].apply(prior_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decided the split method \n",
    "def split_method(data):\n",
    "    split_word=re.split('\\W+',data)\n",
    "    return split_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# the stop words \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words=stopwords.words(\"english\")\n",
    "new_stopping_words = stop_words[:len(stop_words)-36]\n",
    "new_stopping_words.remove(\"not\") \n",
    "new_stopping_words.extend(['would','could',])\n",
    "new_stopping_words.remove(\"but\")\n",
    "type_class=[word.lower() for word in list(WC['Class Name'].unique())[:-3]]\n",
    "new_stopping_words.extend(type_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['intimates',\n",
       " 'dresses',\n",
       " 'pants',\n",
       " 'blouses',\n",
       " 'knits',\n",
       " 'outerwear',\n",
       " 'lounge',\n",
       " 'sweaters',\n",
       " 'skirts',\n",
       " 'fine gauge',\n",
       " 'sleep',\n",
       " 'jackets',\n",
       " 'swim',\n",
       " 'trend',\n",
       " 'jeans',\n",
       " 'legwear',\n",
       " 'shorts',\n",
       " 'layering']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the stop words\n",
    "def remove_stop(data):\n",
    "    data=[word.lower()for word in data]\n",
    "    return [word for word in data if word not in new_stopping_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the number in review\n",
    "def remove_num(data):\n",
    "    return [word for word in data if word.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatizze(data): \n",
    "    newdata= [WordNetLemmatizer().lemmatize(t) for t in data]\n",
    "    return newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(data):\n",
    "    data1=split_method(data)\n",
    "    data2=remove_stop(data1)\n",
    "    data3=remove_num(data2)\n",
    "    final_data=lemmatizze(data3)\n",
    "    return ' '.join(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zheng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "WC['review_clean']=WC['Review Text'].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review Text</th>\n",
       "      <th>review_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Love this dress!  it is sooo pretty.  i happen...</td>\n",
       "      <td>love dress sooo pretty happened find store gla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>high hope dress really wanted work initially o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I love, love, love this jumpsuit. it is fun, f...</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review Text  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...   \n",
       "1  Love this dress!  it is sooo pretty.  i happen...   \n",
       "2  I had such high hopes for this dress and reall...   \n",
       "3  I love, love, love this jumpsuit. it is fun, f...   \n",
       "4  This shirt is very flattering to all due to th...   \n",
       "\n",
       "                                        review_clean  \n",
       "0        absolutely wonderful silky sexy comfortable  \n",
       "1  love dress sooo pretty happened find store gla...  \n",
       "2  high hope dress really wanted work initially o...  \n",
       "3  love love love jumpsuit fun flirty fabulous ev...  \n",
       "4  shirt flattering due adjustable front tie perf...  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC[['Review Text','review_clean']].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# creating new features according to the review contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['length_review']=WC['review_clean'].apply(lambda x : len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number(data):\n",
    "    return ' '.join(re.findall('([0-9]+\\W?[0-9]+)',data))\n",
    "def number_lbs(data):\n",
    "    return ' '.join(re.findall('([0-9]+lb+.?)',data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['number']=WC['Review Text'].apply(number)\n",
    "WC['number_lbs']= WC['Review Text'].apply(number_lbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['have titile']=WC['Title'].notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['have_number']= WC['number']!=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['have_number']=WC['have_number']+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC['have titile']=WC['have titile']+0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Title</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Division Name</th>\n",
       "      <th>Department Name</th>\n",
       "      <th>Class Name</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>length_review</th>\n",
       "      <th>number</th>\n",
       "      <th>number_lbs</th>\n",
       "      <th>have titile</th>\n",
       "      <th>have_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>767</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Initmates</td>\n",
       "      <td>Intimate</td>\n",
       "      <td>Intimates</td>\n",
       "      <td>absolutely wonderful silky sexy comfortable</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love this dress!  it is sooo pretty.  i happen...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>love dress sooo pretty happened find store gla...</td>\n",
       "      <td>27</td>\n",
       "      <td>5'8</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1077</td>\n",
       "      <td>60</td>\n",
       "      <td>Some major design flaws</td>\n",
       "      <td>I had such high hopes for this dress and reall...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>General</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>high hope dress really wanted work initially o...</td>\n",
       "      <td>50</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1049</td>\n",
       "      <td>50</td>\n",
       "      <td>My favorite buy!</td>\n",
       "      <td>I love, love, love this jumpsuit. it is fun, f...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>General Petite</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Pants</td>\n",
       "      <td>love love love jumpsuit fun flirty fabulous ev...</td>\n",
       "      <td>15</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>847</td>\n",
       "      <td>47</td>\n",
       "      <td>Flattering shirt</td>\n",
       "      <td>This shirt is very flattering to all due to th...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>General</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Blouses</td>\n",
       "      <td>shirt flattering due adjustable front tie perf...</td>\n",
       "      <td>16</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Clothing ID  Age                    Title  \\\n",
       "0          767   33                      NaN   \n",
       "1         1080   34                      NaN   \n",
       "2         1077   60  Some major design flaws   \n",
       "3         1049   50         My favorite buy!   \n",
       "4          847   47         Flattering shirt   \n",
       "\n",
       "                                         Review Text  Rating  Recommended IND  \\\n",
       "0  Absolutely wonderful - silky and sexy and comf...       4                0   \n",
       "1  Love this dress!  it is sooo pretty.  i happen...       5                0   \n",
       "2  I had such high hopes for this dress and reall...       3                1   \n",
       "3  I love, love, love this jumpsuit. it is fun, f...       5                0   \n",
       "4  This shirt is very flattering to all due to th...       5                0   \n",
       "\n",
       "   Positive Feedback Count   Division Name Department Name Class Name  \\\n",
       "0                        0       Initmates        Intimate  Intimates   \n",
       "1                        4         General         Dresses    Dresses   \n",
       "2                        0         General         Dresses    Dresses   \n",
       "3                        0  General Petite         Bottoms      Pants   \n",
       "4                        6         General            Tops    Blouses   \n",
       "\n",
       "                                        review_clean  length_review number  \\\n",
       "0        absolutely wonderful silky sexy comfortable              5          \n",
       "1  love dress sooo pretty happened find store gla...             27    5'8   \n",
       "2  high hope dress really wanted work initially o...             50          \n",
       "3  love love love jumpsuit fun flirty fabulous ev...             15          \n",
       "4  shirt flattering due adjustable front tie perf...             16          \n",
       "\n",
       "  number_lbs  have titile  have_number  \n",
       "0                       0            0  \n",
       "1                       0            1  \n",
       "2                       1            0  \n",
       "3                       1            0  \n",
       "4                       1            0  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Building ML mode for recommendation prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the imbalance in train dataset. \n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "def adjust_balance(Xtrain,ytrain):\n",
    "    rus = RandomUnderSampler(replacement=False)\n",
    "    X_train_subsample, y_train_subsample = rus.fit_resample(Xtrain, ytrain)\n",
    "    return X_train_subsample, y_train_subsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vectorization--TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review --- vector \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "def review_to_vector(df_train,df_test):  # 产生df中不同的词的向量表示，一个大型的稀疏矩阵\n",
    "    tf_idf_vectorizer = TfidfVectorizer(max_df=0.95,min_df=0.001)# min_df=0.001 \n",
    "    tf_idf_vectorizer.fit(df_train)\n",
    "    word_names= tf_idf_vectorizer.get_feature_names() \n",
    "    X_train_tf = tf_idf_vectorizer.transform(df_train).toarray()\n",
    "    X_test_tf = tf_idf_vectorizer.transform(df_test).toarray()\n",
    "    return X_train_tf,X_test_tf,word_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 Evaluate different models' performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#同时报告这些参数\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import  f1_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "WC_analysis=WC.copy()\n",
    "WC_analysis=pd.get_dummies(data=WC_analysis, columns=['Department Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "std = MinMaxScaler()\n",
    "WC_analysis[['Age','length_review']] = std.fit_transform(WC_analysis[['Age','length_review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Clothing ID', 'Age', 'Title', 'Review Text', 'Rating',\n",
       "       'Recommended IND', 'Positive Feedback Count', 'Division Name',\n",
       "       'Class Name', 'review_clean', 'length_review', 'number', 'number_lbs',\n",
       "       'have titile', 'have_number', 'Department Name_Bottoms',\n",
       "       'Department Name_Dresses', 'Department Name_Intimate',\n",
       "       'Department Name_Jackets', 'Department Name_Tops',\n",
       "       'Department Name_Trend'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WC_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new feature selected by us. \n",
    "X_list=['Age','length_review','Department Name_Bottoms','Department Name_Dresses', 'Department Name_Intimate',\n",
    "       'Department Name_Jackets', 'Department Name_Tops','Department Name_Trend','review_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=WC_analysis[X_list]\n",
    "y=WC_analysis[['Recommended IND']]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)\n",
    "X_train_b,y_train_b=adjust_balance(X_train,y_train)\n",
    "\n",
    "X_train_idf,X_test_idf,word_names=review_to_vector(X_train_b['review_clean'],X_test['review_clean'])\n",
    "y_train=y_train_b['Recommended IND']\n",
    "y_test=y_test['Recommended IND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5818, 2038)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_task(m,X_test_scaled ,y_test,mn): \n",
    "    predic=m.predict(X_test_scaled)\n",
    "    predict_probs=m.predict_proba(X_test_scaled)[:,1]\n",
    "    fpr,tpr, thr=roc_curve(y_test,predict_probs)\n",
    "    roc_auc=auc(fpr,tpr) \n",
    "    perf_df=pd.DataFrame({\"Precision_Score\":precision_score(y_test,predic),\"Recall_Score\":recall_score(y_test,predic),\n",
    "                       \"F1_Score\":f1_score(y_test,predic) , \"Accuracy\":accuracy_score(y_test,predic),\"AUC\":roc_auc},index=[mn])\n",
    "    return perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=3,max_features='auto',random_state=101)\n",
    "clf.fit(X_train_idf, y_train)\n",
    "feature_importance_=pd.DataFrame(clf.feature_importances_,index=word_names,columns=['feature_importance']).sort_values('feature_importance',ascending=False)\n",
    "feature_importance_20=feature_importance_.head(20)\n",
    "per=classification_task(clf,X_test_idf ,y_test,'GBDT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.059930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfect</th>\n",
       "      <td>0.059524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wanted</th>\n",
       "      <td>0.053419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>0.049622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>disappointed</th>\n",
       "      <td>0.046707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>back</th>\n",
       "      <td>0.043305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looked</th>\n",
       "      <td>0.039993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comfortable</th>\n",
       "      <td>0.037969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>0.031330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return</th>\n",
       "      <td>0.027376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unflattering</th>\n",
       "      <td>0.025319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returning</th>\n",
       "      <td>0.025279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft</th>\n",
       "      <td>0.023015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>returned</th>\n",
       "      <td>0.022203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>little</th>\n",
       "      <td>0.019583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>however</th>\n",
       "      <td>0.019276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfectly</th>\n",
       "      <td>0.018560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>but</th>\n",
       "      <td>0.017408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>huge</th>\n",
       "      <td>0.017103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unfortunately</th>\n",
       "      <td>0.015601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cheap</th>\n",
       "      <td>0.015005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>excited</th>\n",
       "      <td>0.012410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compliment</th>\n",
       "      <td>0.012401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way</th>\n",
       "      <td>0.012197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fabric</th>\n",
       "      <td>0.010189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wear</th>\n",
       "      <td>0.009835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size</th>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bit</th>\n",
       "      <td>0.008192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.007538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>didnot</th>\n",
       "      <td>0.007131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poor</th>\n",
       "      <td>0.006975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comfy</th>\n",
       "      <td>0.006950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>going</th>\n",
       "      <td>0.005949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feminine</th>\n",
       "      <td>0.005895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bought</th>\n",
       "      <td>0.005866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>casual</th>\n",
       "      <td>0.005740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.005305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sadly</th>\n",
       "      <td>0.004736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nice</th>\n",
       "      <td>0.004725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wide</th>\n",
       "      <td>0.004522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beautifully</th>\n",
       "      <td>0.004490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>even</th>\n",
       "      <td>0.004482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thought</th>\n",
       "      <td>0.004410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thin</th>\n",
       "      <td>0.004115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boot</th>\n",
       "      <td>0.003943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>0.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>glad</th>\n",
       "      <td>0.003817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>easy</th>\n",
       "      <td>0.003785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>highly</th>\n",
       "      <td>0.003476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enough</th>\n",
       "      <td>0.003431</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               feature_importance\n",
       "love                     0.059930\n",
       "perfect                  0.059524\n",
       "wanted                   0.053419\n",
       "great                    0.049622\n",
       "disappointed             0.046707\n",
       "back                     0.043305\n",
       "looked                   0.039993\n",
       "comfortable              0.037969\n",
       "not                      0.031330\n",
       "return                   0.027376\n",
       "unflattering             0.025319\n",
       "returning                0.025279\n",
       "soft                     0.023015\n",
       "returned                 0.022203\n",
       "little                   0.019583\n",
       "however                  0.019276\n",
       "perfectly                0.018560\n",
       "but                      0.017408\n",
       "huge                     0.017103\n",
       "unfortunately            0.015601\n",
       "cheap                    0.015005\n",
       "excited                  0.012410\n",
       "compliment               0.012401\n",
       "way                      0.012197\n",
       "fabric                   0.010189\n",
       "wear                     0.009835\n",
       "size                     0.009534\n",
       "bit                      0.008192\n",
       "like                     0.007538\n",
       "didnot                   0.007131\n",
       "poor                     0.006975\n",
       "comfy                    0.006950\n",
       "going                    0.005949\n",
       "feminine                 0.005895\n",
       "bought                   0.005866\n",
       "casual                   0.005740\n",
       "model                    0.005305\n",
       "sadly                    0.004736\n",
       "nice                     0.004725\n",
       "wide                     0.004522\n",
       "beautifully              0.004490\n",
       "even                     0.004482\n",
       "thought                  0.004410\n",
       "thin                     0.004115\n",
       "boot                     0.003943\n",
       "true                     0.003867\n",
       "glad                     0.003817\n",
       "easy                     0.003785\n",
       "highly                   0.003476\n",
       "enough                   0.003431"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importance_.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_.head(50).to_excel('feature_importance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'accent',\n",
       " 'accentuate',\n",
       " 'accentuated',\n",
       " 'accentuates',\n",
       " 'acceptable',\n",
       " 'accessory',\n",
       " 'accidentally',\n",
       " 'accommodate',\n",
       " 'according',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'across',\n",
       " 'acrylic',\n",
       " 'actual',\n",
       " 'actually',\n",
       " 'add',\n",
       " 'added',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'adjust',\n",
       " 'adjustable',\n",
       " 'admit',\n",
       " 'adn',\n",
       " 'adorable',\n",
       " 'adore',\n",
       " 'advertised',\n",
       " 'advice',\n",
       " 'aesthetic',\n",
       " 'afraid',\n",
       " 'ag',\n",
       " 'age',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ahead',\n",
       " 'air',\n",
       " 'airy',\n",
       " 'ala',\n",
       " 'allow',\n",
       " 'allows',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'alteration',\n",
       " 'altered',\n",
       " 'alternative',\n",
       " 'although',\n",
       " 'always',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amount',\n",
       " 'amp',\n",
       " 'ample',\n",
       " 'ankle',\n",
       " 'annoying',\n",
       " 'another',\n",
       " 'anticipated',\n",
       " 'anymore',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'apart',\n",
       " 'appeal',\n",
       " 'appealing',\n",
       " 'appear',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'appears',\n",
       " 'appreciate',\n",
       " 'appropriate',\n",
       " 'aqua',\n",
       " 'area',\n",
       " 'arenot',\n",
       " 'arm',\n",
       " 'armhole',\n",
       " 'armpit',\n",
       " 'around',\n",
       " 'arrival',\n",
       " 'arrive',\n",
       " 'arrived',\n",
       " 'art',\n",
       " 'asap',\n",
       " 'aside',\n",
       " 'asked',\n",
       " 'aspect',\n",
       " 'associate',\n",
       " 'assumed',\n",
       " 'assuming',\n",
       " 'asymmetrical',\n",
       " 'athletic',\n",
       " 'attached',\n",
       " 'attention',\n",
       " 'attracted',\n",
       " 'attractive',\n",
       " 'available',\n",
       " 'average',\n",
       " 'avoid',\n",
       " 'aware',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'awhile',\n",
       " 'awkward',\n",
       " 'awkwardly',\n",
       " 'baby',\n",
       " 'back',\n",
       " 'background',\n",
       " 'backside',\n",
       " 'bad',\n",
       " 'badly',\n",
       " 'bag',\n",
       " 'baggy',\n",
       " 'balance',\n",
       " 'ball',\n",
       " 'ballet',\n",
       " 'balloon',\n",
       " 'band',\n",
       " 'bandeau',\n",
       " 'bare',\n",
       " 'barely',\n",
       " 'based',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'bathing',\n",
       " 'bc',\n",
       " 'beach',\n",
       " 'bead',\n",
       " 'beading',\n",
       " 'beat',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'become',\n",
       " 'bed',\n",
       " 'began',\n",
       " 'begin',\n",
       " 'behind',\n",
       " 'beige',\n",
       " 'believe',\n",
       " 'bell',\n",
       " 'belly',\n",
       " 'belt',\n",
       " 'belted',\n",
       " 'bend',\n",
       " 'beneath',\n",
       " 'best',\n",
       " 'bet',\n",
       " 'better',\n",
       " 'beware',\n",
       " 'beyond',\n",
       " 'bib',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'bikini',\n",
       " 'bill',\n",
       " 'billow',\n",
       " 'billowed',\n",
       " 'billowy',\n",
       " 'bird',\n",
       " 'birthday',\n",
       " 'bit',\n",
       " 'bizarre',\n",
       " 'black',\n",
       " 'blah',\n",
       " 'bland',\n",
       " 'blanket',\n",
       " 'blazer',\n",
       " 'bled',\n",
       " 'blend',\n",
       " 'block',\n",
       " 'blocking',\n",
       " 'blonde',\n",
       " 'blouse',\n",
       " 'blousy',\n",
       " 'blue',\n",
       " 'bodice',\n",
       " 'body',\n",
       " 'bohemian',\n",
       " 'boho',\n",
       " 'bone',\n",
       " 'bonus',\n",
       " 'boo',\n",
       " 'boob',\n",
       " 'boot',\n",
       " 'booty',\n",
       " 'bordeaux',\n",
       " 'boring',\n",
       " 'bother',\n",
       " 'bothered',\n",
       " 'bottom',\n",
       " 'bought',\n",
       " 'bow',\n",
       " 'box',\n",
       " 'boxy',\n",
       " 'boy',\n",
       " 'boyfriend',\n",
       " 'bra',\n",
       " 'brand',\n",
       " 'break',\n",
       " 'breaker',\n",
       " 'breast',\n",
       " 'breath',\n",
       " 'breathable',\n",
       " 'breezy',\n",
       " 'bridal',\n",
       " 'bright',\n",
       " 'brighter',\n",
       " 'bring',\n",
       " 'bringing',\n",
       " 'broad',\n",
       " 'broader',\n",
       " 'broke',\n",
       " 'bronze',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'brunch',\n",
       " 'bu',\n",
       " 'bubble',\n",
       " 'buck',\n",
       " 'build',\n",
       " 'built',\n",
       " 'bulge',\n",
       " 'bulk',\n",
       " 'bulky',\n",
       " 'bum',\n",
       " 'bummed',\n",
       " 'bummer',\n",
       " 'bump',\n",
       " 'bunch',\n",
       " 'bunched',\n",
       " 'bunching',\n",
       " 'burgundy',\n",
       " 'burnt',\n",
       " 'business',\n",
       " 'bust',\n",
       " 'busted',\n",
       " 'bustier',\n",
       " 'busty',\n",
       " 'busy',\n",
       " 'but',\n",
       " 'butt',\n",
       " 'button',\n",
       " 'buttoned',\n",
       " 'buttoning',\n",
       " 'buy',\n",
       " 'buyer',\n",
       " 'buying',\n",
       " 'byron',\n",
       " 'ca',\n",
       " 'cage',\n",
       " 'calf',\n",
       " 'california',\n",
       " 'call',\n",
       " 'called',\n",
       " 'came',\n",
       " 'cami',\n",
       " 'camisole',\n",
       " 'cannot',\n",
       " 'canot',\n",
       " 'cant',\n",
       " 'cap',\n",
       " 'cape',\n",
       " 'capri',\n",
       " 'cardi',\n",
       " 'cardigan',\n",
       " 'care',\n",
       " 'careful',\n",
       " 'carefully',\n",
       " 'carry',\n",
       " 'cart',\n",
       " 'case',\n",
       " 'cashmere',\n",
       " 'casual',\n",
       " 'casually',\n",
       " 'catalog',\n",
       " 'catching',\n",
       " 'caught',\n",
       " 'causal',\n",
       " 'cause',\n",
       " 'caused',\n",
       " 'causing',\n",
       " 'caveat',\n",
       " 'center',\n",
       " 'centered',\n",
       " 'certain',\n",
       " 'certainly',\n",
       " 'chambray',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changed',\n",
       " 'charcoal',\n",
       " 'charm',\n",
       " 'charming',\n",
       " 'chart',\n",
       " 'cheap',\n",
       " 'cheaper',\n",
       " 'cheaply',\n",
       " 'check',\n",
       " 'checked',\n",
       " 'chest',\n",
       " 'chested',\n",
       " 'chic',\n",
       " 'chiffon',\n",
       " 'child',\n",
       " 'chilly',\n",
       " 'chino',\n",
       " 'choice',\n",
       " 'choose',\n",
       " 'chose',\n",
       " 'christmas',\n",
       " 'chunky',\n",
       " 'church',\n",
       " 'cinch',\n",
       " 'cinched',\n",
       " 'city',\n",
       " 'classic',\n",
       " 'classy',\n",
       " 'clean',\n",
       " 'cleaned',\n",
       " 'cleaner',\n",
       " 'cleaning',\n",
       " 'clear',\n",
       " 'clearly',\n",
       " 'cleavage',\n",
       " 'cling',\n",
       " 'clingy',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'closely',\n",
       " 'closer',\n",
       " 'closet',\n",
       " 'closure',\n",
       " 'cloth',\n",
       " 'clothes',\n",
       " 'clothing',\n",
       " 'clung',\n",
       " 'coarse',\n",
       " 'coat',\n",
       " 'cocktail',\n",
       " 'cold',\n",
       " 'colder',\n",
       " 'collar',\n",
       " 'color',\n",
       " 'colored',\n",
       " 'colorful',\n",
       " 'coloring',\n",
       " 'combination',\n",
       " 'combined',\n",
       " 'combo',\n",
       " 'come',\n",
       " 'comfort',\n",
       " 'comfortable',\n",
       " 'comfortably',\n",
       " 'comfy',\n",
       " 'coming',\n",
       " 'comment',\n",
       " 'compare',\n",
       " 'compared',\n",
       " 'comparison',\n",
       " 'complaint',\n",
       " 'complement',\n",
       " 'complete',\n",
       " 'completely',\n",
       " 'complexion',\n",
       " 'compliment',\n",
       " 'complimentary',\n",
       " 'con',\n",
       " 'concept',\n",
       " 'concern',\n",
       " 'concerned',\n",
       " 'confident',\n",
       " 'confused',\n",
       " 'conservative',\n",
       " 'consider',\n",
       " 'considered',\n",
       " 'considering',\n",
       " 'consistently',\n",
       " 'constantly',\n",
       " 'constructed',\n",
       " 'construction',\n",
       " 'continues',\n",
       " 'contrast',\n",
       " 'contrasting',\n",
       " 'control',\n",
       " 'convinced',\n",
       " 'cool',\n",
       " 'cooler',\n",
       " 'coral',\n",
       " 'cord',\n",
       " 'corduroy',\n",
       " 'correct',\n",
       " 'correctly',\n",
       " 'cost',\n",
       " 'costume',\n",
       " 'cotton',\n",
       " 'couldnot',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'cover',\n",
       " 'coverage',\n",
       " 'covered',\n",
       " 'covering',\n",
       " 'cowl',\n",
       " 'cozy',\n",
       " 'crazy',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'created',\n",
       " 'creates',\n",
       " 'crisp',\n",
       " 'crochet',\n",
       " 'crop',\n",
       " 'cropped',\n",
       " 'cross',\n",
       " 'crossed',\n",
       " 'crotch',\n",
       " 'cuff',\n",
       " 'culotte',\n",
       " 'cup',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'curve',\n",
       " 'curvier',\n",
       " 'curvy',\n",
       " 'customer',\n",
       " 'cut',\n",
       " 'cute',\n",
       " 'cuter',\n",
       " 'cutest',\n",
       " 'cutout',\n",
       " 'cycle',\n",
       " 'damaged',\n",
       " 'dance',\n",
       " 'dark',\n",
       " 'darker',\n",
       " 'darling',\n",
       " 'dart',\n",
       " 'date',\n",
       " 'daughter',\n",
       " 'day',\n",
       " 'dd',\n",
       " 'deal',\n",
       " 'debating',\n",
       " 'decent',\n",
       " 'decide',\n",
       " 'decided',\n",
       " 'decision',\n",
       " 'deep',\n",
       " 'defective',\n",
       " 'definite',\n",
       " 'definitely',\n",
       " 'definition',\n",
       " 'degree',\n",
       " 'delicate',\n",
       " 'denim',\n",
       " 'depending',\n",
       " 'depicted',\n",
       " 'describe',\n",
       " 'described',\n",
       " 'description',\n",
       " 'design',\n",
       " 'designed',\n",
       " 'designer',\n",
       " 'desired',\n",
       " 'despite',\n",
       " 'detail',\n",
       " 'detailed',\n",
       " 'detailing',\n",
       " 'developed',\n",
       " 'diagonal',\n",
       " 'didnot',\n",
       " 'didnt',\n",
       " 'difference',\n",
       " 'different',\n",
       " 'differently',\n",
       " 'difficult',\n",
       " 'dinner',\n",
       " 'dip',\n",
       " 'direction',\n",
       " 'dirty',\n",
       " 'disagree',\n",
       " 'disappoint',\n",
       " 'disappointed',\n",
       " 'disappointing',\n",
       " 'disappointment',\n",
       " 'disaster',\n",
       " 'discount',\n",
       " 'display',\n",
       " 'displayed',\n",
       " 'distressed',\n",
       " 'doesnot',\n",
       " 'dog',\n",
       " 'doll',\n",
       " 'dollar',\n",
       " 'dolman',\n",
       " 'done',\n",
       " 'donot',\n",
       " 'dont',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'doubt',\n",
       " 'dowdy',\n",
       " 'down',\n",
       " 'downside',\n",
       " 'drab',\n",
       " 'drape',\n",
       " 'draped',\n",
       " 'drapey',\n",
       " 'draw',\n",
       " 'drawback',\n",
       " 'drawn',\n",
       " 'drawstring',\n",
       " 'dream',\n",
       " 'dreamy',\n",
       " 'dress',\n",
       " 'dressed',\n",
       " 'dressier',\n",
       " 'dressing',\n",
       " 'dressy',\n",
       " 'drew',\n",
       " 'dried',\n",
       " 'droopy',\n",
       " 'drop',\n",
       " 'dropped',\n",
       " 'dry',\n",
       " 'dryer',\n",
       " 'due',\n",
       " 'dull',\n",
       " 'dumpy',\n",
       " 'durable',\n",
       " 'dusty',\n",
       " 'dye',\n",
       " 'earlier',\n",
       " 'early',\n",
       " 'ease',\n",
       " 'easier',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'edge',\n",
       " 'edgy',\n",
       " 'effect',\n",
       " 'effort',\n",
       " 'effortless',\n",
       " 'either',\n",
       " 'elastic',\n",
       " 'elbow',\n",
       " 'elegance',\n",
       " 'elegant',\n",
       " 'element',\n",
       " 'else',\n",
       " 'embroidered',\n",
       " 'embroidery',\n",
       " 'emphasized',\n",
       " 'empire',\n",
       " 'end',\n",
       " 'ended',\n",
       " 'endowed',\n",
       " 'enjoy',\n",
       " 'enormous',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'entirely',\n",
       " 'envisioned',\n",
       " 'errand',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'evening',\n",
       " 'event',\n",
       " 'eventually',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyday',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'exact',\n",
       " 'exactly',\n",
       " 'exaggerated',\n",
       " 'excellent',\n",
       " 'except',\n",
       " 'exception',\n",
       " 'exceptional',\n",
       " 'excess',\n",
       " 'exchange',\n",
       " 'exchanged',\n",
       " 'exchanging',\n",
       " 'excited',\n",
       " 'execution',\n",
       " 'expect',\n",
       " 'expectation',\n",
       " 'expected',\n",
       " 'expecting',\n",
       " 'expensive',\n",
       " 'experience',\n",
       " 'experienced',\n",
       " 'exposed',\n",
       " 'extra',\n",
       " 'extreme',\n",
       " 'extremely',\n",
       " 'eye',\n",
       " 'eyeing',\n",
       " 'eyelet',\n",
       " 'fabric',\n",
       " 'fabulous',\n",
       " 'fact',\n",
       " 'factor',\n",
       " 'faded',\n",
       " 'fair',\n",
       " 'fairly',\n",
       " 'fall',\n",
       " 'falling',\n",
       " 'fan',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fashion',\n",
       " 'fast',\n",
       " 'fat',\n",
       " 'fault',\n",
       " 'faux',\n",
       " 'favor',\n",
       " 'favorite',\n",
       " 'fear',\n",
       " 'feature',\n",
       " 'fee',\n",
       " 'feel',\n",
       " 'feeling',\n",
       " 'fell',\n",
       " 'felt',\n",
       " 'feminine',\n",
       " 'fence',\n",
       " 'fiber',\n",
       " 'figure',\n",
       " 'figured',\n",
       " 'fill',\n",
       " 'finally',\n",
       " 'find',\n",
       " 'finding',\n",
       " 'fine',\n",
       " 'finger',\n",
       " 'finish',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'fitted',\n",
       " 'fitting',\n",
       " 'five',\n",
       " 'fix',\n",
       " 'fixed',\n",
       " 'flair',\n",
       " 'flannel',\n",
       " 'flap',\n",
       " 'flare',\n",
       " 'flared',\n",
       " 'flat',\n",
       " 'flatter',\n",
       " 'flattering',\n",
       " 'flatters',\n",
       " 'flaw',\n",
       " 'fleece',\n",
       " 'flimsy',\n",
       " 'flip',\n",
       " 'flirty',\n",
       " 'floor',\n",
       " 'flop',\n",
       " 'floppy',\n",
       " 'floral',\n",
       " 'floreat',\n",
       " 'florida',\n",
       " 'flow',\n",
       " 'flowed',\n",
       " 'flower',\n",
       " 'flowing',\n",
       " 'flowy',\n",
       " 'flutter',\n",
       " 'fold',\n",
       " 'folded',\n",
       " 'folk',\n",
       " 'followed',\n",
       " 'following',\n",
       " 'fooled',\n",
       " 'foot',\n",
       " 'football',\n",
       " 'forever',\n",
       " 'forgiving',\n",
       " 'form',\n",
       " 'formal',\n",
       " 'forward',\n",
       " 'found',\n",
       " 'four',\n",
       " 'fragile',\n",
       " 'frame',\n",
       " 'fray',\n",
       " 'frayed',\n",
       " 'fraying',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'friday',\n",
       " 'friend',\n",
       " 'fringe',\n",
       " 'front',\n",
       " 'frumpy',\n",
       " 'frustrating',\n",
       " 'ft',\n",
       " 'full',\n",
       " 'fuller',\n",
       " 'fullness',\n",
       " 'fully',\n",
       " 'fun',\n",
       " 'functional',\n",
       " 'funky',\n",
       " 'funny',\n",
       " 'fur',\n",
       " 'fyi',\n",
       " 'gal',\n",
       " 'gap',\n",
       " 'gape',\n",
       " 'gaping',\n",
       " 'gapping',\n",
       " 'garment',\n",
       " 'gather',\n",
       " 'gathered',\n",
       " 'gathering',\n",
       " 'gauze',\n",
       " 'gave',\n",
       " 'general',\n",
       " 'generally',\n",
       " 'generous',\n",
       " 'generously',\n",
       " 'gentle',\n",
       " 'gently',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'giant',\n",
       " 'gift',\n",
       " 'gigantic',\n",
       " 'girl',\n",
       " 'give',\n",
       " 'given',\n",
       " 'giving',\n",
       " 'glad',\n",
       " 'glass',\n",
       " 'glove',\n",
       " 'go',\n",
       " 'going',\n",
       " 'gold',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'gorgeous',\n",
       " 'got',\n",
       " 'gotten',\n",
       " 'gown',\n",
       " 'grab',\n",
       " 'grabbed',\n",
       " 'granted',\n",
       " 'gray',\n",
       " 'great',\n",
       " 'green',\n",
       " 'grey',\n",
       " 'ground',\n",
       " 'guess',\n",
       " 'gym',\n",
       " 'ha',\n",
       " 'hadnot',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'halter',\n",
       " 'hand',\n",
       " 'handle',\n",
       " 'hang',\n",
       " 'hanger',\n",
       " 'hanging',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'happening',\n",
       " 'happens',\n",
       " 'happier',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hardly',\n",
       " 'hate',\n",
       " 'hated',\n",
       " 'havenot',\n",
       " 'hd',\n",
       " 'head',\n",
       " 'headed',\n",
       " 'heart',\n",
       " 'heat',\n",
       " 'heather',\n",
       " 'heavier',\n",
       " 'heavy',\n",
       " 'heel',\n",
       " 'heeled',\n",
       " 'height',\n",
       " 'held',\n",
       " 'help',\n",
       " 'helped',\n",
       " 'hem',\n",
       " 'hemline',\n",
       " 'hemmed',\n",
       " 'hemming',\n",
       " 'hence',\n",
       " 'hesitant',\n",
       " 'hesitate',\n",
       " 'hesitated',\n",
       " 'hi',\n",
       " 'hidden',\n",
       " 'hide',\n",
       " 'high',\n",
       " 'higher',\n",
       " 'highly',\n",
       " 'hint',\n",
       " 'hip',\n",
       " 'hit',\n",
       " 'hitting',\n",
       " 'hold',\n",
       " 'holding',\n",
       " 'hole',\n",
       " 'holiday',\n",
       " 'home',\n",
       " 'honestly',\n",
       " 'hood',\n",
       " 'hoodie',\n",
       " 'hook',\n",
       " 'hope',\n",
       " 'hoped',\n",
       " 'hopeful',\n",
       " 'hopefully',\n",
       " 'hoping',\n",
       " 'horizontal',\n",
       " 'horrible',\n",
       " 'horribly',\n",
       " 'horse',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'hourglass',\n",
       " 'house',\n",
       " 'however',\n",
       " 'hte',\n",
       " 'hue',\n",
       " 'hug',\n",
       " 'huge',\n",
       " 'hugging',\n",
       " 'humid',\n",
       " 'hung',\n",
       " 'hunt',\n",
       " 'husband',\n",
       " 'hyphen',\n",
       " 'idea',\n",
       " 'ideal',\n",
       " 'ill',\n",
       " 'illusion',\n",
       " 'im',\n",
       " 'image',\n",
       " 'imagine',\n",
       " 'imagined',\n",
       " 'immediately',\n",
       " 'imperfection',\n",
       " 'impossible',\n",
       " 'impractical',\n",
       " 'impressed',\n",
       " 'impression',\n",
       " 'inch',\n",
       " 'included',\n",
       " 'including',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'indeed',\n",
       " 'initially',\n",
       " 'inner',\n",
       " 'inseam',\n",
       " 'inside',\n",
       " 'instantly',\n",
       " 'instead',\n",
       " 'instruction',\n",
       " 'interest',\n",
       " 'interesting',\n",
       " 'intricate',\n",
       " 'investment',\n",
       " 'iron',\n",
       " 'ironing',\n",
       " 'ish',\n",
       " 'isnot',\n",
       " 'issue',\n",
       " 'itch',\n",
       " 'itchy',\n",
       " 'item',\n",
       " 'ivory',\n",
       " 'jacket',\n",
       " 'jean',\n",
       " 'jersey',\n",
       " 'jewelry',\n",
       " 'job',\n",
       " 'jogger',\n",
       " 'joke',\n",
       " 'jumper',\n",
       " 'jumpsuit',\n",
       " 'justice',\n",
       " 'justify',\n",
       " 'keep',\n",
       " 'keeper',\n",
       " 'keeping',\n",
       " 'kept',\n",
       " 'key',\n",
       " 'kid',\n",
       " 'kimono',\n",
       " 'kind',\n",
       " 'kinda',\n",
       " 'knee',\n",
       " 'knew',\n",
       " 'knit',\n",
       " 'knitted',\n",
       " 'knot',\n",
       " 'know',\n",
       " 'knowing',\n",
       " 'known',\n",
       " 'la',\n",
       " 'label',\n",
       " 'lace',\n",
       " 'lack',\n",
       " 'lacy',\n",
       " 'lady',\n",
       " 'laid',\n",
       " 'large',\n",
       " 'larger',\n",
       " 'lars',\n",
       " 'last',\n",
       " 'late',\n",
       " 'lately',\n",
       " 'later',\n",
       " 'laughing',\n",
       " 'lavender',\n",
       " 'lay',\n",
       " 'layer',\n",
       " 'layered',\n",
       " 'layering',\n",
       " 'lazy',\n",
       " 'lb',\n",
       " 'le',\n",
       " 'leaf',\n",
       " 'lean',\n",
       " 'least',\n",
       " 'leather',\n",
       " 'leave',\n",
       " 'leaving',\n",
       " 'left',\n",
       " 'leg',\n",
       " 'legging',\n",
       " 'length',\n",
       " 'let',\n",
       " 'level',\n",
       " 'lie',\n",
       " 'life',\n",
       " 'lift',\n",
       " 'lifted',\n",
       " 'light',\n",
       " 'lighter',\n",
       " 'lightweight',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'likely',\n",
       " 'liking',\n",
       " 'line',\n",
       " 'lined',\n",
       " 'linen',\n",
       " 'liner',\n",
       " 'lingerie',\n",
       " 'lining',\n",
       " 'lint',\n",
       " 'list',\n",
       " 'listed',\n",
       " 'listened',\n",
       " 'literally',\n",
       " 'little',\n",
       " 'live',\n",
       " 'local',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'loop',\n",
       " 'loose',\n",
       " 'loosely',\n",
       " 'looser',\n",
       " 'lose',\n",
       " 'lost',\n",
       " 'lot',\n",
       " 'lounge',\n",
       " 'lounging',\n",
       " 'love',\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB=MultinomialNB()\n",
    "NB.fit(X_train_idf, y_train)\n",
    "perm_NB=classification_task(NB,X_test_idf ,y_test,'NB')\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train_idf, y_train)\n",
    "perm_LR=classification_task(lr,X_test_idf ,y_test,'LR')\n",
    "rfc=RandomForestClassifier(n_estimators=100,random_state=101)\n",
    "rfc.fit(X_train_idf, y_train)\n",
    "perm_RFC=classification_task(rfc,X_test_idf ,y_test,'RFC')\n",
    "per_model=pd.concat([per,perm_NB,perm_RFC,perm_LR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "per_model.to_excel('after_stop_words.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add new variables. --age,lenght_review and the department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add new feature\n",
    "X_train_add=[]\n",
    "X_test_add=[]\n",
    "for i in range(X_train_idf.shape[0]):\n",
    "    X_train_add.append(np.append(X_train_idf[i],X_train_b.iloc[i,:-1]))\n",
    "for i in range(X_test_idf.shape[0]):\n",
    "    X_test_add.append(np.append(X_test_idf[i],X_test.iloc[i,:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5818, 2067)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_idf=np.array(X_train_add)\n",
    "X_test_idf=np.array(X_test_add)\n",
    "X_train_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=3,max_features='auto',random_state=101)\n",
    "clf.fit(X_train_idf, y_train)\n",
    "per_GBDT_add=classification_task(clf,X_test_idf ,y_test,'GBDT_add')\n",
    "NB=MultinomialNB()\n",
    "NB.fit(X_train_idf, y_train)\n",
    "perm_NB_add=classification_task(NB,X_test_idf ,y_test,'NB_add')\n",
    "lr=LogisticRegression()\n",
    "lr.fit(X_train_idf, y_train)\n",
    "perm_LR_add=classification_task(lr,X_test_idf ,y_test,'LR_add')\n",
    "rfc=RandomForestClassifier(n_estimators=100,random_state=101)\n",
    "rfc.fit(X_train_idf, y_train)\n",
    "perm_RFC_add=classification_task(rfc,X_test_idf ,y_test,'RFC_add')\n",
    "per_model_add=pd.concat([per_GBDT_add,perm_NB_add,perm_RFC_add,perm_LR_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision_Score</th>\n",
       "      <th>Recall_Score</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBDT_add</th>\n",
       "      <td>0.458119</td>\n",
       "      <td>0.821309</td>\n",
       "      <td>0.588165</td>\n",
       "      <td>0.798145</td>\n",
       "      <td>0.883796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NB_add</th>\n",
       "      <td>0.509425</td>\n",
       "      <td>0.861577</td>\n",
       "      <td>0.640274</td>\n",
       "      <td>0.830094</td>\n",
       "      <td>0.917994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFC_add</th>\n",
       "      <td>0.500993</td>\n",
       "      <td>0.846477</td>\n",
       "      <td>0.629445</td>\n",
       "      <td>0.825088</td>\n",
       "      <td>0.908377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR_add</th>\n",
       "      <td>0.531522</td>\n",
       "      <td>0.869966</td>\n",
       "      <td>0.659879</td>\n",
       "      <td>0.842609</td>\n",
       "      <td>0.922203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Precision_Score  Recall_Score  F1_Score  Accuracy       AUC\n",
       "GBDT_add         0.458119      0.821309  0.588165  0.798145  0.883796\n",
       "NB_add           0.509425      0.861577  0.640274  0.830094  0.917994\n",
       "RFC_add          0.500993      0.846477  0.629445  0.825088  0.908377\n",
       "LR_add           0.531522      0.869966  0.659879  0.842609  0.922203"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_model_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imporve the performance of model by high-parameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'n_estimators': [50,100, 150, 200],\n",
    "        'learning_rate': [0.05, 0.10, 0.20],\n",
    "        'max_depth': [3,4,5],\n",
    "        'min_samples_split':[20,30,40,50]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=GradientBoostingClassifier(max_features='auto',\n",
       "                                                        random_state=101),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.05, 0.1, 0.2],\n",
       "                                        'max_depth': [3, 4, 5],\n",
       "                                        'min_samples_split': [20, 30, 40, 50],\n",
       "                                        'n_estimators': [50, 100, 150, 200]},\n",
       "                   random_state=1001, scoring='recall', verbose=3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbdt_class=GradientBoostingClassifier(max_features='auto',random_state=101)\n",
    "\n",
    "random_search = RandomizedSearchCV(gbdt_class,param_distributions=params, n_iter=50, scoring='recall', n_jobs=-1, cv=5, verbose=3, random_state=1001 )\n",
    "\n",
    "random_search.fit(X_train_idf, y_train)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_best = GradientBoostingClassifier(n_estimators=200,max_depth=5,min_samples_split=50,learning_rate=0.1,max_features='auto',random_state=101)\n",
    "clf_best.fit(X_train_idf, y_train)\n",
    "per_best=classification_task(clf_best,X_test_idf ,y_test,'GBDT_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision_Score</th>\n",
       "      <th>Recall_Score</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GBDT_best</th>\n",
       "      <td>0.50603</td>\n",
       "      <td>0.844799</td>\n",
       "      <td>0.632935</td>\n",
       "      <td>0.828033</td>\n",
       "      <td>0.907551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Precision_Score  Recall_Score  F1_Score  Accuracy       AUC\n",
       "GBDT_best          0.50603      0.844799  0.632935  0.828033  0.907551"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "\n",
      " Best estimator:\n",
      "RandomForestClassifier(max_depth=7, min_samples_split=50, n_estimators=190,\n",
      "                       random_state=101)\n"
     ]
    }
   ],
   "source": [
    "params_rfc = {\n",
    "        'n_estimators': [100, 130,160,190],\n",
    "        'max_depth': [3,5,7],\n",
    "        'min_samples_split':[30,40,50]\n",
    "        }\n",
    "RFC_tune=RandomForestClassifier(random_state=101)\n",
    "random_search = RandomizedSearchCV(RFC_tune,param_distributions=params_rfc, n_iter=50, scoring='recall', n_jobs=-1, cv=5, verbose=3, random_state=1001 )\n",
    "random_search.fit(X_train_idf, y_train)\n",
    "print('\\n Best estimator:')\n",
    "print(random_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_best=RandomForestClassifier(n_estimators=160,random_state=101)\n",
    "rfc_best.fit(X_train_idf, y_train)\n",
    "perm_rfc_best=classification_task(rfc_best,X_test_idf ,y_test,'RFC_best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision_Score</th>\n",
       "      <th>Recall_Score</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RFC_best</th>\n",
       "      <td>0.501957</td>\n",
       "      <td>0.860738</td>\n",
       "      <td>0.634116</td>\n",
       "      <td>0.825677</td>\n",
       "      <td>0.912709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Precision_Score  Recall_Score  F1_Score  Accuracy       AUC\n",
       "RFC_best         0.501957      0.860738  0.634116  0.825677  0.912709"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm_rfc_best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
